"""
å°ç£æœ¬åœ°åŒ–æ–°èç¯©é¸å™¨
ç§»æ¤è‡ª n8n workflow çš„ Code3 ç¯€é»

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ™ºèƒ½è©•åˆ†ç³»çµ±
2. å°ç£è¦–è§’å„ªå…ˆ
3. ä¾†æºå¹³è¡¡ç­–ç•¥
"""

import logging
from datetime import datetime, timedelta
from typing import List, Dict
import re

logger = logging.getLogger(__name__)

# ============================================
# ç¯©é¸é…ç½®ï¼ˆèˆ‡ n8n Code3 å®Œå…¨ä¸€è‡´ï¼‰
# ============================================

FILTERS = {
    'sources': {
        # === å°ç£æœ¬åœ°ä¾†æº ===
        'technews': {
            'priority_keywords': [
                # AI ç›¸é—œ
                'AI', 'äººå·¥æ™ºæ…§', 'ChatGPT', 'Claude', 'Gemini',
                'ç”Ÿæˆå¼', 'LLM', 'å¤§å‹èªè¨€æ¨¡å‹',
                # å°ç£é—œéµå­—
                'å°ç©é›»', 'TSMC', 'è¯ç™¼ç§‘', 'é´»æµ·', 'è¯ç¢©', 'å®ç¢',
                'å°ç£', 'Taiwan', 'æ•¸ä½ç™¼å±•éƒ¨', 'è³‡ç­–æœƒ',
                # å¯¦ç”¨å·¥å…·
                'å·¥å…·', 'App', 'æ‡‰ç”¨ç¨‹å¼', 'é–‹æº', 'å…è²»'
            ],
            'exclude': [
                'è‚¡åƒ¹', 'è²¡å ±', 'ç‡Ÿæ”¶', 'æ³•èªªæœƒ',
                'ä½µè³¼', 'æŠ•è³‡', 'åŸºé‡‘'
            ],
            'max_items': 12,
            'base_score': 8
        },
        
        'ithome': {
            'priority_keywords': [
                'AI', 'è³‡å®‰', 'Cloud', 'é›²ç«¯', 'DevOps',
                'é–‹ç™¼', 'Python', 'JavaScript', 'API',
                'å¾®è»Ÿ', 'Google', 'AWS', 'Azure',
                'ä¼æ¥­æ‡‰ç”¨', 'æ•¸ä½è½‰å‹', 'è‡ªå‹•åŒ–'
            ],
            'exclude': [
                'ç ”è¨æœƒ', 'è«–å£‡', 'æ‹›æ¨™', 'æ¡è³¼'
            ],
            'max_items': 10,
            'base_score': 7
        },
        
        'inside': {
            'priority_keywords': [
                'startup', 'æ–°å‰µ', 'AI', 'å‰µæ–°', 'Web3',
                'NFT', 'å€å¡Šéˆ', 'Fintech', 'é‡‘èç§‘æŠ€',
                'é›»å•†', 'SaaS', 'B2B', 'B2C',
                'ä½¿ç”¨è€…é«”é©—', 'UX', 'ç”¢å“è¨­è¨ˆ'
            ],
            'exclude': [
                'å‹Ÿè³‡', 'ç¨®å­è¼ª', 'Series', 'IPO'
            ],
            'max_items': 8,
            'base_score': 6
        },
        
        # === åœ‹éš›ä¾†æº ===
        'hackernews': {
            'priority_keywords': [
                'AI', 'ChatGPT', 'Claude', 'Gemini', 'OpenAI',
                'tool', 'app', 'browser', 'Python', 'npm'
            ],
            'exclude': [
                'CVE-2025', 'CVSS', 'vulnerability', 'ransomware'
            ],
            'max_items': 8,
            'base_score': 0
        },
        
        'techcrunch': {
            'priority_keywords': [
                'AI', 'ChatGPT', 'OpenAI', 'Anthropic',
                'app', 'tool', 'feature', 'launch'
            ],
            'exclude': [
                'raises', 'funding', 'valuation', 'layoffs'
            ],
            'max_items': 6,
            'base_score': 0
        },
        
        'openai': {
            'priority_keywords': ['GPT', 'API', 'model', 'release'],
            'exclude': [],
            'max_items': 5,
            'base_score': 15
        },
        
        'arstechnica': {
            'priority_keywords': [
                'AI', 'science', 'research', 'quantum', 'space'
            ],
            'exclude': ['gaming', 'review', 'streaming'],
            'max_items': 4,
            'base_score': 0
        },
        
        'bair': {
            'priority_keywords': ['research', 'paper', 'algorithm'],
            'exclude': [],
            'max_items': 3,
            'base_score': 3
        }
    },
    
    # å°ç£æ°‘çœ¾ç‰¹åˆ¥é—œæ³¨çš„é—œéµå­—
    'taiwan_interests': [
        # æœ¬åœŸä¼æ¥­èˆ‡ç”¢æ¥­
        'åŠå°é«”', 'æ™¶ç‰‡', 'æ™¶åœ“', 'ICè¨­è¨ˆ', 'å°æ¸¬',
        'é›»å‹•è»Š', 'å„²èƒ½', 'ç¶ èƒ½', 'å¤ªé™½èƒ½', 'é¢¨é›»',
        
        # å°ç£ç›¸é—œåœ‹éš›æ–°è
        'Taiwan', 'å°ç£', 'Taipei', 'å°åŒ—',
        'Asia', 'äºæ´²', 'æ±å—äº', 'ASEAN',
        
        # å¯¦ç”¨æ€§é«˜çš„å…§å®¹
        'æ•™å­¸', 'æ‡¶äººåŒ…', 'æ¯”è¼ƒ', 'æ¨è–¦', 'å…è²»',
        'ä¸­æ–‡', 'ç¹é«”', 'åœ¨åœ°åŒ–', 'æœ¬åœŸåŒ–',
        
        # ç†±é–€æ‡‰ç”¨
        'LINE', 'Instagram', 'YouTube', 'æŠ–éŸ³', 'TikTok',
        'è¡—å£', 'PChome', 'è¦çš®', 'momo'
    ],
    
    # å…¨çƒè¶¨å‹¢ä½†å°ç£ç‰¹åˆ¥é—œæ³¨
    'global_taiwan_focus': [
        'NVIDIA', 'AMD', 'Intel',
        'Apple', 'iPhone',
        'ä¾›æ‡‰éˆ', 'supply chain',
        'ä¸­ç¾', 'US-China', 'æ™¶ç‰‡æˆ°'
    ],
    
    'must_keep_phrases': [
        'å°ç©é›»', 'TSMC',
        'æ•¸ä½ç™¼å±•éƒ¨',
        'ChatGPT é–‹æ”¾å°ç£',
        'Google å°ç£',
        'Microsoft å°ç£'
    ]
}

# ä¾†æºä¸­æ–‡æ¨™ç±¤
SOURCE_LABELS = {
    'technews': 'ğŸ‡¹ğŸ‡¼ ç§‘æŠ€æ–°å ±',
    'ithome': 'ğŸ‡¹ğŸ‡¼ iThome',
    'inside': 'ğŸ‡¹ğŸ‡¼ INSIDE',
    'hackernews': 'ğŸŒ Hacker News',
    'techcrunch': 'ğŸŒ TechCrunch',
    'arstechnica': 'ğŸŒ Ars Technica',
    'openai': 'ğŸ¤– OpenAI',
    'bair': 'ğŸ“ Berkeley AI'
}


def calculate_relevance(item: Dict) -> int:
    """
    è¨ˆç®—æ–°èçš„ç›¸é—œæ€§åˆ†æ•¸
    
    Args:
        item: æ–°èé …ç›®
        
    Returns:
        ç›¸é—œæ€§åˆ†æ•¸
    """
    title = item.get('title', '').lower()
    content = item.get('content', '').lower()
    link = item.get('link', '')
    full_text = f"{title} {content}"
    
    source = item.get('source', 'unknown')
    config = FILTERS['sources'].get(source, {
        'priority_keywords': [],
        'exclude': [],
        'base_score': 0
    })
    
    score = config.get('base_score', 0)
    
    # 1. å¿…é ˆä¿ç•™
    for phrase in FILTERS['must_keep_phrases']:
        if phrase.lower() in full_text:
            return 100
    
    # 2. æ’é™¤é—œéµå­—
    for keyword in config.get('exclude', []):
        if keyword.lower() in full_text:
            score -= 5
    
    # 3. ä¾†æºå„ªå…ˆé—œéµå­—
    for keyword in config.get('priority_keywords', []):
        keyword_lower = keyword.lower()
        if keyword_lower in title:
            score += 10
        elif keyword_lower in content:
            score += 5
    
    # 4. å°ç£èˆˆè¶£é—œéµå­—ï¼ˆé¡å¤–åŠ åˆ†ï¼‰
    for keyword in FILTERS['taiwan_interests']:
        if keyword.lower() in full_text:
            score += 4
    
    # 5. å…¨çƒä½†å°ç£é—œæ³¨çš„ä¸»é¡Œ
    for keyword in FILTERS['global_taiwan_focus']:
        if keyword.lower() in full_text:
            score += 6
    
    # 6. ç‰¹æ®Šè™•ç†
    is_taiwan_source = source in ['technews', 'ithome', 'inside']
    is_international_source = source in ['hackernews', 'techcrunch', 'openai']
    
    if is_taiwan_source:
        score += 5
        if 'åœ‹éš›' in full_text or 'global' in full_text:
            score += 8
    
    if is_international_source:
        if 'taiwan' in full_text or 'asia' in full_text:
            score += 10
    
    # 7. å¯¦ç”¨æ€§åŠ åˆ†
    practical_keywords = ['æ•™å­¸', 'tutorial', 'guide', 'å¯¦æ¸¬', 'è©•æ¸¬', 'æ¯”è¼ƒ']
    for keyword in practical_keywords:
        if keyword in title:
            score += 7
    
    # 8. å…§å®¹é•·åº¦
    if len(content) > 300:
        score += 2
    if len(content) > 500:
        score += 2
    
    return score


def filter_and_score_news(all_news: List[Dict], target_date: str) -> List[Dict]:
    """
    ç¯©é¸å’Œè©•åˆ†æ–°è
    
    Args:
        all_news: æ‰€æœ‰æ–°èåˆ—è¡¨
        target_date: ç›®æ¨™æ—¥æœŸ
        
    Returns:
        ç¯©é¸å¾Œçš„æ–°èåˆ—è¡¨
    """
    logger.info("ğŸ” é–‹å§‹ç¯©é¸æ–°è...")
    
    # è§£æç›®æ¨™æ—¥æœŸ
    target_dt = datetime.strptime(target_date, '%Y-%m-%d')
    yesterday = target_dt - timedelta(days=1)
    yesterday_str = yesterday.strftime('%Y-%m-%d')
    
    # åˆ†çµ„è™•ç†
    grouped = {source: [] for source in FILTERS['sources'].keys()}
    grouped['unknown'] = []
    
    for item in all_news:
        # æª¢æŸ¥æ—¥æœŸ
        pub_date = item.get('isoDate', '')
        if pub_date:
            try:
                pub_dt = datetime.fromisoformat(pub_date.replace('Z', '+00:00'))
                if pub_dt.strftime('%Y-%m-%d') != yesterday_str:
                    continue
            except:
                continue
        
        # è¨ˆç®—åˆ†æ•¸
        score = calculate_relevance(item)
        source = item.get('source', 'unknown')
        
        # æ·»åŠ é¡å¤–è³‡è¨Š
        enriched_item = {
            **item,
            'relevance_score': score,
            'source_label': SOURCE_LABELS.get(source, 'ğŸ“° å…¶ä»–')
        }
        
        if source in grouped:
            grouped[source].append(enriched_item)
        else:
            grouped['unknown'].append(enriched_item)
    
    # æ’åºå’Œé™åˆ¶
    taiwan_news = []
    international_news = []
    
    for source, items in grouped.items():
        if not items or source == 'unknown':
            continue
        
        config = FILTERS['sources'].get(source, {})
        max_items = config.get('max_items', 5)
        
        # æ’åºä¸¦ç¯©é¸
        filtered = sorted(items, key=lambda x: x['relevance_score'], reverse=True)
        filtered = [item for item in filtered if item['relevance_score'] > 0]
        filtered = filtered[:max_items]
        
        # åˆ†é¡æœ¬åœ°èˆ‡åœ‹éš›
        if source in ['technews', 'ithome', 'inside']:
            taiwan_news.extend(filtered)
        else:
            international_news.extend(filtered)
        
        logger.info(f"  {SOURCE_LABELS.get(source, source)}: {len(items)} â†’ {len(filtered)}")
    
    # æ··åˆæ’åºç­–ç•¥ï¼šç¢ºä¿æœ¬åœ°èˆ‡åœ‹éš›æ–°èå¹³è¡¡
    final_items = []
    max_length = max(len(taiwan_news), len(international_news))
    
    for i in range(max_length):
        if i < len(taiwan_news):
            final_items.append(taiwan_news[i])
        if i < len(international_news):
            final_items.append(international_news[i])
    
    # æœ€çµ‚æŒ‰åˆ†æ•¸é‡æ’ï¼ˆä½†ä¿æŒä¸€å®šå¤šæ¨£æ€§ï¼‰
    final_items.sort(key=lambda x: (
        # å…ˆæŒ‰åˆ†æ•¸åˆ†çµ„
        -1 if x['relevance_score'] > 20 else (-2 if x['relevance_score'] > 10 else -3),
        # åŒçµ„å…§æŒ‰åˆ†æ•¸æ’åº
        -x['relevance_score']
    ))
    
    # çµ±è¨ˆå ±å‘Š
    logger.info("\nğŸ“Š ç¯©é¸çµæœç¸½è¦½ï¼š")
    logger.info("ã€å°ç£æ–°èã€‘")
    for source in ['technews', 'ithome', 'inside']:
        count = len([item for item in final_items if item['source'] == source])
        logger.info(f"  {SOURCE_LABELS[source]}: {count} å‰‡")
    
    logger.info("\nã€åœ‹éš›æ–°èã€‘")
    for source in ['hackernews', 'techcrunch', 'openai', 'arstechnica', 'bair']:
        count = len([item for item in final_items if item['source'] == source])
        logger.info(f"  {SOURCE_LABELS[source]}: {count} å‰‡")
    
    taiwan_count = len([i for i in final_items if i['source'] in ['technews', 'ithome', 'inside']])
    international_count = len(final_items) - taiwan_count
    
    logger.info(f"\n{'=' * 40}")
    logger.info(f"âœ… æœ€çµ‚ä¿ç•™: {len(final_items)} å‰‡")
    logger.info(f"  - æœ¬åœ°: {taiwan_count} å‰‡")
    logger.info(f"  - åœ‹éš›: {international_count} å‰‡")
    logger.info(f"{'=' * 40}\n")
    
    return final_items
