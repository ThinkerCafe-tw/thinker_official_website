"""
RSS Feed è®€å–æ¨¡çµ„
å¾å¤šå€‹ä¾†æºè®€å– RSS feeds
"""

import feedparser
import logging
from datetime import datetime
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger(__name__)

# RSS ä¾†æºé…ç½®
RSS_SOURCES = {
    'hackernews': 'https://feeds.feedburner.com/TheHackersNews',
    'techcrunch': 'https://techcrunch.com/feed/',
    'arstechnica': 'http://feeds.arstechnica.com/arstechnica/index/',
    'openai': 'https://openai.com/news/rss.xml',
    'bair': 'https://bair.berkeley.edu/blog/feed.xml',
    'technews': 'https://technews.tw/feed/',
    'ithome': 'https://www.ithome.com.tw/rss',
}


def fetch_single_feed(source_name: str, url: str) -> List[Dict]:
    """
    è®€å–å–®ä¸€ RSS feed
    
    Args:
        source_name: ä¾†æºåç¨±
        url: RSS feed URL
        
    Returns:
        æ–°èåˆ—è¡¨
    """
    try:
        logger.info(f"  ğŸ“¡ è®€å– {source_name}...")
        feed = feedparser.parse(url)
        
        if feed.bozo:
            logger.warning(f"  âš ï¸  {source_name} RSS æ ¼å¼æœ‰å•é¡Œ")
        
        news_items = []
        for entry in feed.entries:
            try:
                # æå–æ–°èè³‡è¨Š
                item = {
                    'title': entry.get('title', ''),
                    'link': entry.get('link', ''),
                    'content': entry.get('summary', entry.get('description', '')),
                    'pubDate': entry.get('published', entry.get('updated', '')),
                    'isoDate': entry.get('published_parsed', entry.get('updated_parsed', None)),
                    'source': source_name
                }
                
                # è½‰æ›æ—¥æœŸæ ¼å¼
                if item['isoDate']:
                    try:
                        dt = datetime(*item['isoDate'][:6])
                        item['isoDate'] = dt.isoformat()
                    except:
                        item['isoDate'] = ''
                
                news_items.append(item)
                
            except Exception as e:
                logger.warning(f"  âš ï¸  è™•ç† {source_name} çš„æŸå‰‡æ–°èæ™‚å‡ºéŒ¯: {str(e)}")
                continue
        
        logger.info(f"  âœ… {source_name}: è®€å– {len(news_items)} å‰‡")
        return news_items
        
    except Exception as e:
        logger.error(f"  âŒ è®€å– {source_name} å¤±æ•—: {str(e)}")
        return []


def fetch_all_rss_feeds(today_date: str) -> List[Dict]:
    """
    ä¸¦è¡Œè®€å–æ‰€æœ‰ RSS feeds
    
    Args:
        today_date: ä»Šæ—¥æ—¥æœŸï¼ˆç”¨æ–¼æ—¥èªŒï¼‰
        
    Returns:
        æ‰€æœ‰æ–°èçš„åˆ—è¡¨
    """
    all_news = []
    
    # ä½¿ç”¨ ThreadPoolExecutor ä¸¦è¡Œè®€å–
    with ThreadPoolExecutor(max_workers=7) as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        future_to_source = {
            executor.submit(fetch_single_feed, name, url): name
            for name, url in RSS_SOURCES.items()
        }
        
        # æ”¶é›†çµæœ
        for future in as_completed(future_to_source):
            source_name = future_to_source[future]
            try:
                news_items = future.result()
                all_news.extend(news_items)
            except Exception as e:
                logger.error(f"âŒ {source_name} è®€å–ä»»å‹™å¤±æ•—: {str(e)}")
    
    logger.info(f"ğŸ“Š ç¸½å…±è®€å– {len(all_news)} å‰‡æ–°è")
    return all_news
